{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03a13ac9-8270-44a7-bebe-6e0d0e8e923e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import autograd.numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from autograd import grad \n",
    "from autograd import hessian\n",
    "import pandas as pd\n",
    "\n",
    "def model(x,w):\n",
    "    a = w[0] + np.dot(x.T,w[1:])\n",
    "    return a.T\n",
    "\n",
    "def reg_softmax(w):\n",
    "    cost = np. sum (np.log (1 + np .exp(-y * model(x, w))))\n",
    "    #cost += L * np.sum(w[1:]**2)\n",
    "    return cost / float(np.size(y))\n",
    "\n",
    "def missed_class(w,x,y):\n",
    "    y_p= np.sign(model(x,w))\n",
    "    missed = np.count_nonzero(y-y_p)\n",
    "    return missed\n",
    "    \n",
    "def gradient_descent(g,w,its,alpha,plot):\n",
    "\n",
    "    cost_history = []\n",
    "    w_history = []\n",
    "    mis_class_history = []\n",
    "    \n",
    "    #find gradient of cost function\n",
    "    gradient = grad(g)\n",
    "\n",
    "    #store inital values\n",
    "    cost_history.append(g(w))\n",
    "    w_history.append(w)\n",
    "    \n",
    "    \n",
    "    for i in range(its+1):\n",
    "\n",
    "        #find gradient\n",
    "        grad_eval = gradient(w)\n",
    "\n",
    "        #update w\n",
    "        w_new = w - alpha*grad_eval\n",
    "        w = w_new\n",
    "\n",
    "        #add to history \n",
    "        cost_history.append(g(w))\n",
    "        w_history.append(w)\n",
    "\n",
    "    w_f = w_history[-1]\n",
    "\n",
    "    if (plot ==1):\n",
    "        \n",
    "        #plot cost history\n",
    "        \n",
    "        #print(f\"Final weight vector: {w_f}\")\n",
    "        \n",
    "        print(f'final cost: {cost_history[-1]}')\n",
    "        plt.figure()\n",
    "        plt.plot(cost_history)\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Cost')\n",
    "        plt.title('Cost history')\n",
    "        plt.show()\n",
    "        \n",
    "        mis_class_history = [missed_class(w, x, y) for w in w_history]\n",
    "        valid_mis_class_h = [missed_class(w, x_valid, y_valid) for w in w_history]\n",
    "        #print(f'training misclassifications: {mis_class_history[-1]}')\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.plot(mis_class_history,label = 'training')\n",
    "        plt.plot(valid_mis_class_h,label = 'validation')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Misclassification')\n",
    "        plt.title('Misclassification history')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "    return w_history,cost_history,w_f \n",
    "\n",
    "def standard_normalise(x):\n",
    "    #NxP\n",
    "    rows = x.shape[0]\n",
    "    means = np.mean(x,axis= 1)\n",
    "    std_dvs = np.std(x,axis = 1)\n",
    "    shape = np.shape(x)\n",
    "    normalised = np.zeros(shape)\n",
    "\n",
    "    for i in range(rows):\n",
    "        if (std_dvs[i] > 0):\n",
    "            normalised[i,:] = (x[i,:] - means[i]) / float(std_dvs[i])\n",
    "    \n",
    "    return normalised,means,std_dvs\n",
    "\n",
    "def std_normalise_new_data(x,means,std_dvs):\n",
    "    return (x-means)/float(std_dvs)\n",
    "\n",
    "def un_norm(x,means,std_dvs):\n",
    "    return((x*std_dvs)+means)\n",
    "    \n",
    "\n",
    "def train_split(x,y,split):\n",
    "    #data should be NxP shape \n",
    "    \n",
    "    # randomly split training and validation data\n",
    "    num_samples = x.shape[1]\n",
    "    indices = np.random.permutation(num_samples)\n",
    "    \n",
    "    # Determine the split point\n",
    "    split_point = int(num_samples * split)\n",
    "    \n",
    "    # Split the indices into two parts\n",
    "    train_indices = indices[:split_point]\n",
    "    test_indices = indices[split_point:]\n",
    "    #print(train_indices)\n",
    "    #print(test_indices)\n",
    "    \n",
    "    # Separate the data into training and testing sets\n",
    "    x_train = x[:, train_indices]\n",
    "    y_train = y[:,train_indices]\n",
    "    x_test = x[:, test_indices]\n",
    "    y_test = y[:,test_indices]\n",
    "\n",
    "    return x_train,y_train,x_test,y_test\n",
    "\n",
    "def evaluate(w,x,y,verbose):\n",
    "    #generate accuracy, specificity, precision \n",
    "    y_p = np.sign(model(x,w))\n",
    "    \n",
    "    size = y.size\n",
    "    \n",
    "    TP= 0\n",
    "    FP= 0\n",
    "    FN= 0\n",
    "    TN= 0\n",
    "\n",
    "    for i in range(size):\n",
    "\n",
    "        if (y_p[0,i] == 1 and y[0,i]==1 ): TP+=1\n",
    "        elif (y_p[0,i] ==1 and y[0,i] ==-1): FP+=1\n",
    "        elif(y_p[0,i] ==-1 and y[0,i] ==1): FN+=1\n",
    "        elif(y_p[0,i] ==-1 and y[0,i] ==-1):TN+=1\n",
    "\n",
    "    if (TP+FP == 0 ):\n",
    "        #print('no P predicted')\n",
    "        pre = 0\n",
    "    else:\n",
    "        #precision\n",
    "        pre = round(100*(TP/(TP+FP)),2)\n",
    "        \n",
    "    if (TN+TN == 0):\n",
    "        #print('no H predicted')\n",
    "        spc = 0\n",
    "    else:\n",
    "        #specificity\n",
    "        spc = round(100*(TN/(TN+FN)),2)\n",
    "        \n",
    "    #accuracy\n",
    "    acc = round(100*((TP+TN) /size),2)\n",
    "\n",
    "\n",
    "    if (verbose == 1 ): \n",
    "        print(f'accuracy: {acc}%')\n",
    "        print(f'precision: {pre}%')\n",
    "        print(f'specifity: {spc}%')\n",
    "        #print(f'FN: {FN}')\n",
    "        \n",
    "    return acc,pre,spc\n",
    "\n",
    "\n",
    "def bag_eval(y,y_p,verbose):\n",
    "    size = y.size\n",
    "    \n",
    "    TP= 0\n",
    "    FP= 0\n",
    "    FN= 0\n",
    "    TN= 0\n",
    "\n",
    "    for i in range(size):\n",
    "\n",
    "        if (y_p[0,i] == 1 and y[0,i]==1 ): TP+=1\n",
    "        elif (y_p[0,i] ==1 and y[0,i] ==-1): FP+=1\n",
    "        elif(y_p[0,i] ==-1 and y[0,i] ==1): FN+=1\n",
    "        elif(y_p[0,i] ==-1 and y[0,i] ==-1):TN+=1\n",
    "\n",
    "\n",
    "    #accuracy\n",
    "    acc = round(100*((TP+TN) /size),2)\n",
    "    #specificity\n",
    "    spc = round(100*(TN/(TN+FN)),2)\n",
    "    #precision\n",
    "    pre = round(100*(TP/(TP+FP)),2)\n",
    "\n",
    "    if (verbose == 1 ): \n",
    "        print(f'accuracy: {acc}%')\n",
    "        print(f'precision: {pre}%')\n",
    "        print(f'specifity: {spc}%')\n",
    "        print(f'FN: {FN}')\n",
    "        \n",
    "    return acc,pre,spc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def LR_gradient_descent(g,w,its,alpha,plot):\n",
    "\n",
    "    cost_history = []\n",
    "    w_history = []\n",
    "    mis_class_history = []\n",
    "    \n",
    "    #find gradient of cost function\n",
    "    gradient = grad(g)\n",
    "\n",
    "    #store inital values\n",
    "    cost_history.append(g(w))\n",
    "    w_history.append(w)\n",
    "    \n",
    "    \n",
    "    for i in range(its+1):\n",
    "\n",
    "        #find gradient\n",
    "        grad_eval = gradient(w)\n",
    "\n",
    "        #update w\n",
    "        w_new = w - alpha*grad_eval\n",
    "        w = w_new\n",
    "\n",
    "        #add to history \n",
    "        cost_history.append(g(w))\n",
    "        w_history.append(w)\n",
    "\n",
    "    w_f = w_history[-1]\n",
    "\n",
    "    mis_class_history = [missed_class(w, x, y) for w in w_history]\n",
    "    valid_mis_class_h = [missed_class(w, x_valid, y_valid) for w in w_history]\n",
    "\n",
    "    if (plot ==1):\n",
    "        \n",
    "        print(f'final cost: {cost_history[-1]}')\n",
    "        plt.figure()\n",
    "        plt.plot(cost_history)\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Cost')\n",
    "        plt.title('Cost history')\n",
    "        plt.show()\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.plot(mis_class_history,label = 'training')\n",
    "        plt.plot(valid_mis_class_h,label = 'validation')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Misclassification')\n",
    "        plt.title('Misclassification history')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    valid_mis_class_min_index = np.argmin(valid_mis_class_h)\n",
    "    optimal_w = w_history[valid_mis_class_min_index]\n",
    "        \n",
    "    return w_history,cost_history,w_f,optimal_w\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
