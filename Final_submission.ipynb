{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "514a5a7d-3f8a-4a8f-9039-abea2993b551",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## setup and import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11f65bf4-9cd4-41f4-8cd2-55236a1ded63",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import autograd.numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from autograd import grad \n",
    "from autograd import hessian\n",
    "import pandas as pd\n",
    "from scipy.stats import mode\n",
    "data = pd.read_csv('data.csv')\n",
    "#import standard functions \n",
    "%run basic_functions.ipynb\n",
    "%run poly_k_functions.ipynb\n",
    "%run RBF_functions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec16db6-5d67-4c46-97c0-264a7ef6b671",
   "metadata": {},
   "source": [
    "## import and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29c08707-72fd-460b-977e-0dc614580cfa",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 174\n",
      "# Alzheimer: 89 , 51.0 %\n",
      "# Healthy: 85 , 49.0 %\n"
     ]
    }
   ],
   "source": [
    "#extract task 1 features\n",
    "task_one = data.iloc[:, 1:19]\n",
    "\n",
    "#extract diagnosis \n",
    "diag = data.iloc[:,-1]\n",
    "diag = diag.map({'P': 1, 'H' : -1})\n",
    "task_one = task_one.to_numpy().T\n",
    "diag = diag.to_numpy().T\n",
    "diag = diag.reshape((1,174))\n",
    "\n",
    "#all task data \n",
    "all_task = data.iloc[:,1:-1]\n",
    "all_task = all_task.to_numpy().T\n",
    "all_task_norm,means,std = standard_normalise(all_task)\n",
    "\n",
    "#some stats on the data\n",
    "count = np.size(diag)\n",
    "no_alz = np.count_nonzero(diag ==1)\n",
    "no_healthy = np.count_nonzero(diag == -1)\n",
    "\n",
    "print(f'Dataset size: {count}')\n",
    "print(f'# Alzheimer: {no_alz} , {100*round(no_alz/count,2)} %')\n",
    "print(f'# Healthy: {no_healthy} , {100*round(no_healthy/count,2)} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e12895e-ef8d-4153-889f-97364cf322d8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Logistic regression classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091f83ba-5e7b-4d3d-a810-1f2283be5c16",
   "metadata": {},
   "source": [
    "Create a 5 fold data split on training data, optimise each split with gradient descent, do retrospective 'early stopping'by taking the final weights of the model as the ones which produce the lowest validation error during optimisation. Take the 3 best most accurate models per 5-fold cross validation run. Run these top 3 models with the test data, then bag the result (modal output) to calculate accuracy, precision and specifcity. Perform this 20 times, with randomised train (which includes validation) and test data, take a mean of performance staistics to provide an overall evaulation of logistic classification for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d676066-280f-4ea7-8286-8a3fa8c44905",
   "metadata": {},
   "outputs": [],
   "source": [
    "runs =20\n",
    "run_verbose =0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0be0d167-cfff-4c1c-a031-88965786684c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "//////////////////////////////\n",
      "Training and testing over\n",
      "Overall classifier performance:\n",
      "Mean accuracy: 75.86\n",
      "Standard deviation: 3.99\n",
      "Mean precision: 82.29\n",
      "Mean specificity: 71.01\n",
      "//////////////////////////////\n"
     ]
    }
   ],
   "source": [
    "g= reg_softmax\n",
    "alpha = 0.01\n",
    "its = 5000\n",
    "plot = 0\n",
    "\n",
    "runs_acc = []\n",
    "runs_pre = []\n",
    "runs_spc = []\n",
    "\n",
    "for j in range(runs):\n",
    "\n",
    "    if (run_verbose ==1):\n",
    "        print(f'Run: {R}')\n",
    "\n",
    "    #set x train and test\n",
    "    x_train,y_train,x_test,y_test = train_split(all_task_norm,diag,0.8)\n",
    "    \n",
    "    #constants for k-fold\n",
    "    fold_parts = 5\n",
    "    # generate indices of non-overlapping k fold sections\n",
    "    num_samples = x_train.shape[1]\n",
    "    indices = np.arange(num_samples)\n",
    "    sub_indices = np.array_split(indices,fold_parts)\n",
    "    \n",
    "    acc_h = []\n",
    "    w_h = []\n",
    "    for i in range(fold_parts):\n",
    "            #split training and validation data\n",
    "            x_train_fold = np.delete(x_train,sub_indices[i],axis = 1)\n",
    "            y_train_fold = np.delete(y_train,sub_indices[i],axis = 1)\n",
    "            x_valid_fold = x_train[:,sub_indices[i]]\n",
    "            y_valid_fold = y_train[:,sub_indices[i]]\n",
    "\n",
    "            #set x and y so functions can access them\n",
    "            x=x_train_fold\n",
    "            y=y_train_fold #accessed in grad descent in cost function \n",
    "            x_valid = x_valid_fold\n",
    "            y_valid = y_valid_fold\n",
    "\n",
    "            #train\n",
    "            w_init = np.random.randn(451,1)\n",
    "            w_history,cost_history,w_f,w_optimal = LR_gradient_descent(g,w_init,its,alpha,plot=0)\n",
    "    \n",
    "            acc,pre,spc = evaluate(w_optimal,x_valid_fold,y_valid_fold,0)\n",
    "            w_h.append(w_optimal)\n",
    "            acc_h.append(acc)\n",
    "    \n",
    "    #take 3 highest accuracy models out of the 5 and bag modal result\n",
    "    best_index = np.argsort(acc_h)[::-1][:3]\n",
    "    length = np.shape(x_test)[1]\n",
    "    y_p_sub = np.zeros((3,length))\n",
    "    for i in range(3):\n",
    "        y_p_sub[i] = np.sign(model(x_test,w_h[best_index[i]]))\n",
    "    y_p = mode(y_p_sub,axis=0)[0].reshape((1,length))\n",
    "    acc,pre,spc = bag_eval(y_test,y_p,verbose =0)\n",
    "    \n",
    "    runs_acc.append(acc)\n",
    "    runs_pre.append(pre)\n",
    "    runs_spc.append(spc)\n",
    "\n",
    "    if (run_verbose ==1): print(f'Run accuracy: {acc}')\n",
    "\n",
    "print(f'//////////////////////////////')\n",
    "print(f'Training and testing over')\n",
    "print(f'Overall classifier performance:')\n",
    "print(f'Mean accuracy: {round(np.mean(runs_acc),2)}')\n",
    "print(f'Standard deviation: {round(np.std(runs_acc),2)}')\n",
    "print(f'Mean precision: {round(np.mean(runs_pre),2)}')\n",
    "print(f'Mean specificity: {round(np.mean(runs_spc),2)}')\n",
    "print(f'//////////////////////////////')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8f6a47-7013-48d5-997c-08201ece6464",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Cross validation of polynomial kernel method with regularisation, to find optimal regularisation parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c55327e-b757-41e7-82f1-e4093f41e339",
   "metadata": {},
   "source": [
    "For each step in regularisation parameter sweep, split training data into 5 folds, optimise model for each fold configuration, take mean accuracy of all models. The optimal regularisation parameter is the one which produces the highest mean accuracy of the 5-fold configurations. Optimise a model on all training data, using the optimal parameter found, test with test data, find accuracy. During this final optimisation, take the model which produces the lowesest testing validation error, which is not neccesarily the final model at the end of optimisation. Overall classisification accuracy is the mean of the test accuracy over all runs (20). This system uses polynomial kernel, D = 3.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1da09015-f4d6-4afc-b3d1-2766498c4a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = 20\n",
    "run_verbose = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "383c4862-df0c-4a75-a7e0-be7aaaa2cee7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "//////////////////////////////\n",
      "Training and testing over\n",
      "Overall classifier performance:\n",
      "Mean accuracy: 86.14\n",
      "Standard deviation: 4.17\n",
      "Mean precision: 90.02\n",
      "Mean specificity: 83.98\n",
      "Optimal L: 3.3333\n",
      "//////////////////////////////\n"
     ]
    }
   ],
   "source": [
    "runs_acc = []\n",
    "runs_pre = []\n",
    "runs_spc = []\n",
    "\n",
    "#gradient descent parameters\n",
    "g= reg_k_softmax\n",
    "L_set = np.linspace(0,5,10)\n",
    "D = 3\n",
    "C = 1\n",
    "its = 1000\n",
    "alpha = 0.1\n",
    "\n",
    "for R in range(runs):\n",
    "    \n",
    "    if (run_verbose ==1):\n",
    "        print(f'Run: {R}')\n",
    "\n",
    "    #generate training and testing split\n",
    "    x_train,y_train,x_test,y_test = train_split(all_task_norm,diag,0.8)\n",
    "    \n",
    "    #constants for k-fold\n",
    "    fold_parts = 5\n",
    "    # generate indices of non-overlapping k fold sections\n",
    "    num_samples = x_train.shape[1]\n",
    "    indices = np.arange(num_samples)\n",
    "    sub_indices = np.array_split(indices,fold_parts)\n",
    "    \n",
    "    overall_acc = []\n",
    "\n",
    "    for j in range(len(L_set)):\n",
    "        L  = L_set[j]\n",
    "        \n",
    "        acc_h = []\n",
    "        spc_h = []\n",
    "        pre_h = []\n",
    "        \n",
    "        #perform 5 fold cross evlaution to find performance of model\n",
    "        z_init_set = np.random.randn(200,1)\n",
    "        for i in range(fold_parts):\n",
    "            #split training and validation data\n",
    "            x_train_fold = np.delete(x_train,sub_indices[i],axis = 1)\n",
    "            y_train_fold = np.delete(y_train,sub_indices[i],axis = 1)\n",
    "            x_valid_fold = x_train[:,sub_indices[i]]\n",
    "            y_valid_fold = y_train[:,sub_indices[i]]\n",
    "            \n",
    "            y=y_train_fold #accessed in grad descent in cost function \n",
    "            y_valid = y_valid_fold\n",
    "    \n",
    "            #generate z_init\n",
    "            train_size = np.shape(x_train_fold)[1]\n",
    "            z_init = z_init_set[:train_size+1]\n",
    "    \n",
    "            #generate kernel and valid kernel matrix\n",
    "            H = poly_k_matrix(x_train_fold,D,C)\n",
    "            H_valid = make_H_valid(x_train_fold,x_valid_fold)\n",
    "            \n",
    "            #run grad descent for model, record accuracy etc\n",
    "            w_history,cost_history,z_f,optimal_z = kernel_gradient_descent(g,z_init,its,alpha,plot=0)\n",
    "            acc,pre,spc = evaluate(z_f,H_valid,y_valid_fold,verbose=0)\n",
    "    \n",
    "            #store evaluation history of k-fold\n",
    "            acc_h.append(acc)\n",
    "    \n",
    "        #find mean accuracy of whole k-fold evaluation\n",
    "        acc= np.mean(acc_h)\n",
    "        overall_acc.append(acc)\n",
    "    \n",
    "    #identify optimal L Value\n",
    "    L_optimal_index = np.argmax(overall_acc)\n",
    "    L_optimal = L_set[L_optimal_index]\n",
    "    \n",
    "    # train whole traning data and test with test data\n",
    "    train_size = np.shape(x_train)[1]\n",
    "\n",
    "    z_init = np.random.randn(train_size+1,1)\n",
    "    y = y_train\n",
    "    y_valid = y_test\n",
    "    \n",
    "    #generate kernel and test kernel matrix\n",
    "    L =L_optimal\n",
    "    H = poly_k_matrix(x_train,D,C)\n",
    "    H_valid = make_H_valid(x_train,x_test)\n",
    "    w_history,cost_history,z_f,optimal_z = kernel_gradient_descent(g,z_init,its,alpha,plot=0)\n",
    "    acc,pre,spc = evaluate(optimal_z,H_valid,y_valid,verbose=0)\n",
    "    runs_acc.append(acc)\n",
    "    runs_pre.append(pre)\n",
    "    runs_spc.append(spc)\n",
    "       \n",
    "    if (run_verbose ==1):\n",
    "        print(f'Run accuracy: {acc}')\n",
    "\n",
    "print(f'//////////////////////////////')\n",
    "print(f'Training and testing over')\n",
    "print(f'Overall classifier performance:')\n",
    "print(f'Mean accuracy: {round(np.mean(runs_acc),2)}')\n",
    "print(f'Standard deviation: {round(np.std(runs_acc),2)}')\n",
    "print(f'Mean precision: {round(np.mean(runs_pre),2)}')\n",
    "print(f'Mean specificity: {round(np.mean(runs_spc),2)}')\n",
    "#what was the value of L found in the last run, just for reference?\n",
    "print(f'Optimal L: {round(L_optimal,4)}')\n",
    "print(f'//////////////////////////////')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2a5374-6786-4ae3-a5ba-fb84d593bff0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Cross validation of RBF kernel "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78753bf9-fd0a-40ae-88f4-6bf4930a4726",
   "metadata": {},
   "source": [
    "Same as above, but using RBF kernel instead of polynomial kernel. No regulariser. Performing sweep of values for beta (sometimes called gamma)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b72fcfa8-4657-4e01-a12c-90d33618107b",
   "metadata": {},
   "outputs": [],
   "source": [
    "runs  = 20\n",
    "run_verbose = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9243e505-7160-41bd-a3a1-dd94d4e8172d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "//////////////////////////////\n",
      "Training and testing over\n",
      "Overall classifier performance:\n",
      "Mean accuracy: 77.57\n",
      "Standard deviation: 5.37\n",
      "Mean precision: 76.16\n",
      "Mean specificity: 80.82\n",
      "//////////////////////////////\n"
     ]
    }
   ],
   "source": [
    "runs_acc = []\n",
    "runs_pre = []\n",
    "runs_spc = []\n",
    "\n",
    "\n",
    "#gradient descent parameters\n",
    "g= k_softmax\n",
    "its = 1000\n",
    "alpha = 0.1\n",
    "steps = 18\n",
    "betas = np.linspace(0.0005,0.01,steps)\n",
    "\n",
    "for R in range(runs):\n",
    "    \n",
    "    if (run_verbose ==1):\n",
    "        print(f'Run: {R}')\n",
    "   \n",
    "    #generate training and testing split\n",
    "    x_train,y_train,x_test,y_test = train_split(all_task_norm,diag,0.8)\n",
    "    \n",
    "    #constants for k-fold\n",
    "    fold_parts = 5\n",
    "    # generate indices of non-overlapping k fold sections\n",
    "    num_samples = x_train.shape[1]\n",
    "    indices = np.arange(num_samples)\n",
    "    sub_indices = np.array_split(indices,fold_parts)\n",
    "    \n",
    "    overall_acc = []\n",
    "\n",
    "    for j in range(len(betas)):\n",
    "        beta  = betas[j]\n",
    "        \n",
    "        acc_h = []\n",
    "        spc_h = []\n",
    "        pre_h = []\n",
    "        \n",
    "        #perform 5 fold cross evlaution to find performance of model\n",
    "        z_init_set = np.random.randn(200,1)\n",
    "        for i in range(fold_parts):\n",
    "            #split training and validation data\n",
    "            x_train_fold = np.delete(x_train,sub_indices[i],axis = 1)\n",
    "            y_train_fold = np.delete(y_train,sub_indices[i],axis = 1)\n",
    "            x_valid_fold = x_train[:,sub_indices[i]]\n",
    "            y_valid_fold = y_train[:,sub_indices[i]]\n",
    "            \n",
    "            y=y_train_fold #accessed in grad descent in cost function \n",
    "            y_valid = y_valid_fold\n",
    "    \n",
    "            #generate z_init\n",
    "            \n",
    "            train_size = np.shape(x_train_fold)[1]\n",
    "            z_init = z_init_set[:train_size+1]\n",
    "            \n",
    "            #generate kernel and valid kernel matrix\n",
    "            H = RBF_matrix(x_train_fold,beta)\n",
    "            H_valid = make_H_RBF_valid(x_train_fold,x_valid_fold,beta)\n",
    "            \n",
    "            #run grad descent for model, record accuracy etc\n",
    "            w_history,cost_history,z_f,optimal_z = kernel_gradient_descent(g,z_init,its,alpha,plot=0)\n",
    "            acc,pre,spc = evaluate(z_f,H_valid,y_valid_fold,verbose=0)\n",
    "    \n",
    "            #store evaluation history of k-fold\n",
    "            acc_h.append(acc)\n",
    "    \n",
    "        #find mean accuracy of whole k-fold evaluation\n",
    "        acc= np.mean(acc_h)\n",
    "        overall_acc.append(acc)\n",
    "\n",
    "    #identify optimal beta Value\n",
    "    beta_optimal_index = np.argmax(overall_acc)\n",
    "    beta_optimal = betas[L_optimal_index]\n",
    "    \n",
    "    # train whole traning data and test with test data\n",
    "    train_size = np.shape(x_train)[1]\n",
    "    z_init = np.random.randn(train_size+1,1)\n",
    "    y = y_train\n",
    "    y_valid = y_test\n",
    "    \n",
    "    #generate kernel and test kernel matrix, take model at point that is produces lowest test error \n",
    "    beta = beta_optimal\n",
    "    H = RBF_matrix(x_train,beta_optimal)\n",
    "    H_valid = make_H_RBF_valid(x_train,x_test,beta)\n",
    "    w_history,cost_history,z_f,optimal_z = kernel_gradient_descent(g,z_init,its,alpha,plot=0)\n",
    "    acc,pre,spc = evaluate(optimal_z,H_valid,y_valid,verbose=0)\n",
    "    runs_acc.append(acc)\n",
    "    runs_pre.append(pre)\n",
    "    runs_spc.append(spc)\n",
    "       \n",
    "    if (run_verbose ==1):\n",
    "        print(f'Run accuracy: {acc}')\n",
    "\n",
    "print(f'//////////////////////////////')\n",
    "print(f'Training and testing over')\n",
    "print(f'Overall classifier performance:')\n",
    "print(f'Mean accuracy: {round(np.mean(runs_acc),2)}')\n",
    "print(f'Standard deviation: {round(np.std(runs_acc),2)}')\n",
    "print(f'Mean precision: {round(np.mean(runs_pre),2)}')\n",
    "print(f'Mean specificity: {round(np.mean(runs_spc),2)}')\n",
    "print(f'//////////////////////////////')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d3cede-1edd-434b-b236-8ae184b67611",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## MLP - boosting "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82351427-774a-4776-8d99-fed175627ca8",
   "metadata": {},
   "source": [
    "Boost a neural network one unit at a time, choose model with number of units that has lowest validation misclassifications. Then train a NN with this optimal number of units, using all trian data and test with test data. \n",
    "Could be imprived by traing using k-fold cross validation system then bagging output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8f7d271-9385-4f59-9084-ad6af20aa475",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run NN_functions.ipynb\n",
    "#this overwrites some functions used previously, so restart kernel if you want to run the previous classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7f9d4d3-8cdf-48b7-83df-da12d8a466b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_verbose = 0\n",
    "runs =  20\n",
    "max_units = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79120eb2-8d7a-4465-a904-71678c5b5c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "//////////////////////////////\n",
      "Training and testing over\n",
      "Overall classifier performance:\n",
      "Mean accuracy: 59.57\n",
      "Standard deviation: 8.86\n",
      "Mean precision: 59.46\n",
      "Mean specificity: 60.93\n",
      "//////////////////////////////\n"
     ]
    }
   ],
   "source": [
    "runs_acc = []\n",
    "runs_pre = []\n",
    "runs_spc = []\n",
    "for R in range(runs):\n",
    "\n",
    "    if(NN_verbose == 1): print(f'Run: {R}')\n",
    "    \n",
    "    overall_x_train,overall_y_train,x_test,y_test = train_split(all_task_norm,diag,0.8)\n",
    "    x_train,y_train,x_valid,y_valid = train_split(overall_x_train,overall_y_train,0.75)\n",
    "    \n",
    "    #set NN parameters\n",
    "    U_1 = 1\n",
    "    C =1\n",
    "    N=450\n",
    "    layer_sizes = [N, U_1, C]\n",
    "    #theta_init is always a one unit NN\n",
    "    theta_init = network_initializer(layer_sizes,1)\n",
    "\n",
    "    # run for up to 1 to 30 units\n",
    "    system_valid_misclass = []\n",
    "    system_train_misclass = []\n",
    "    \n",
    "    #gradient descent parameters\n",
    "    its =1000\n",
    "    alpha = 1\n",
    "    g=reg_softmax\n",
    "    \n",
    "    x = x_train\n",
    "    y = y_train\n",
    "    \n",
    "    #run system with one unit, genenerate first set w_f\n",
    "    theta_init = network_initializer(layer_sizes,1)\n",
    "    w_history,cost_history,w_f,train_misclass = NN_gradient_descent(g,theta_init,its,alpha,plot=0)\n",
    "    w_set = w_f\n",
    "    \n",
    "    valid_misclass = missed_class(w_set, x_valid, y_valid)\n",
    "    \n",
    "    system_valid_misclass.append(valid_misclass)\n",
    "    system_train_misclass.append(train_misclass)\n",
    "    \n",
    "    # change cost function\n",
    "    g = boost_softmax\n",
    "    \n",
    "    \n",
    "    #run boosting, adding 1 unit at a time \n",
    "    for i in range(1,max_units):\n",
    "        \n",
    "        #set w_set \n",
    "    \n",
    "        #print(f'# units: {i+1}')\n",
    "        \n",
    "        #run to generate new w_f\n",
    "        #theta_inti is start values for new unit\n",
    "        theta_init = network_initializer(layer_sizes,1)\n",
    "        w_history,cost_history,w_f,train_misclass = NN_gradient_descent(g,theta_init,its,alpha,plot=0)\n",
    "    \n",
    "        # system to update weights of system incrementally:\n",
    "        \n",
    "        #append internal weights\n",
    "        new_w_0 = np.append(w_set[0], w_f[0], axis=2)\n",
    "        \n",
    "        #sum external non-touching weights\n",
    "        w_set[1][0] = w_set[1][0] + w_f[1][0]\n",
    "        \n",
    "        #append external weights\n",
    "        new_w_1 = np.append(w_set[1],w_f[1][1])\n",
    "        new_w_1 = new_w_1.reshape((np.size(new_w_1),1))\n",
    "    \n",
    "        #create new w_set\n",
    "        w_set = [new_w_0, new_w_1]\n",
    "    \n",
    "        valid_misclass = missed_class(w_set, x_valid, y_valid)\n",
    "        #print(f'validation misclassifications: {valid_misclass}')\n",
    "        \n",
    "        # append training and validation misclasses to array\n",
    "        system_valid_misclass.append(valid_misclass)\n",
    "        system_train_misclass.append(train_misclass)\n",
    "\n",
    "\n",
    "    #choose system with least valid misclass\n",
    "    best_no_unit = np.argmin(system_valid_misclass)+1\n",
    "    \n",
    "    if (NN_verbose == 1):\n",
    "        #display training and validation error \n",
    "        x_axis = np.arange(1,max_units+1)\n",
    "        plt.figure()\n",
    "        plt.plot(x_axis,system_train_misclass,label = 'training')\n",
    "        plt.plot(x_axis,system_valid_misclass,label = 'validation')\n",
    "        plt.xlabel('number of neural network units')\n",
    "        plt.ylabel('misclassifications')\n",
    "        plt.title(f'Run number: {R}')\n",
    "        plt.legend()\n",
    "        print(f'no_units with lowest misclass = {best_no_unit}')\n",
    "\n",
    "\n",
    "\n",
    "    #train all training data and test with test data\n",
    "    x = overall_x_train\n",
    "    y = overall_y_train\n",
    "    x_valid = x_test\n",
    "    y_valid = y_test\n",
    "    \n",
    "    U_1 = best_no_unit\n",
    "    layer_sizes = [N, U_1, C]\n",
    "    theta_init = network_initializer(layer_sizes,1)\n",
    "\n",
    "    #train on all training data\n",
    "    w_history,cost_history,w_f,train_misclass = NN_gradient_descent(g,theta_init,its,alpha,plot=0)\n",
    "    \n",
    "    #evaluate testing data\n",
    "    acc,pre,spc = evaluate(w_f,x_test,y_test,verbose=0)\n",
    "\n",
    "    if(NN_verbose ==1 ):\n",
    "        print(f'Run accuracy: {acc}')\n",
    "    \n",
    "    runs_acc.append(acc)\n",
    "    runs_pre.append(pre)\n",
    "    runs_spc.append(spc)\n",
    "\n",
    "\n",
    "print(f'//////////////////////////////')\n",
    "print(f'Training and testing over')\n",
    "print(f'Overall classifier performance:')\n",
    "print(f'Mean accuracy: {round(np.mean(runs_acc),2)}')\n",
    "print(f'Standard deviation: {round(np.std(runs_acc),2)}')\n",
    "print(f'Mean precision: {round(np.mean(runs_pre),2)}')\n",
    "print(f'Mean specificity: {round(np.mean(runs_spc),2)}')\n",
    "print(f'//////////////////////////////')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
